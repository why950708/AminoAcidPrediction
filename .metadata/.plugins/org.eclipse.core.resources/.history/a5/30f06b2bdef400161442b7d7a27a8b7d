package lab2;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Paths;
import java.util.*;
import java.util.stream.Stream;

/**
 * Created by Zirui Tao on 2/12/2017.
 */
public class Lab2 {
    static int batchNum = 1;
    static int acidNum = 21;
    static int padNum = 8;
    static int windowSize = 2*padNum + 1;
    static final int EPOCH_NUM = 1000;
   // static final int RAW_INPUT_LAYER_NUM = windowSize;
    static final int input_type_Nums = 21;
    static final int INPUT_LAYER_NUM = windowSize;
    static final int HIDDEN_LAYER_NUM = 7;
    static final int OUTPUT_LAYER_NUM = 5;
    static final int ENSEMBLE_ANN_NUM = 3;
   /* static List<netPerceptron> input_layer;
    static List<netPerceptron> hidden_layer;
    static List<netPerceptron> output_layer;*/
    static double alpha = 0.1;
    static double decay = 0.1*alpha/EPOCH_NUM;
    static double beta = 0.01;
    public static void main(String[] args) throws IOException {
        String train_FileName = args[0];
        //String tune_FileName = args[1];
        List<Strian> train = ParseFile(train_FileName);
        List<Strian> removeList= new ArrayList<>();
        List<Strian>tune  = new ArrayList<Strian>();
        List<Strian>test  = new ArrayList<Strian>();

        // split index into train , tune and test set
        for (int i = 5; i < train.size(); i++) {
            if(i % 5 ==0) {
                tune.add(train.get(i));
                removeList.add(train.get(i));
                if(i + 1 < train.size()){
                    test.add(train.get(i + 1));
                    removeList.add(train.get(i + 1));
                }
            }
        }

        // remove overlaps
        for (Strian strian : removeList) {
            train.remove(strian);
        }
        System.out.println("train sets size:" + train.size());
        System.out.println("tune sets size:" + tune.size());
        System.out.println("test sets size:" + test.size());
       //Test several times for the same settings
        for(int z =0; z<10; z++)
        {
        	
        	
        	 List<ANN> nl = new ArrayList<>();
             for (int i = 0; i < ENSEMBLE_ANN_NUM; i++) {
             ANN network1 = new ANN();
             network1.dropout(2);
             network1.dropout(1);
             nl.add(network1);
             }

                 int count = 0;
                 final double origin_alpha = alpha;
                 double prev_tune_accuracy = 0;
                 int i =0;
                 for (i = 0 ; i < EPOCH_NUM ; i++){
                     // set the appropriate learning rate: decay as time goes
                     alpha = alpha*1/(1 + decay * i);
                     // shuffle the order of list
                     Collections.shuffle(train);
                     for (ANN ann : nl) {
                         // train
                      ann.training(train);
                     }
                     double tune_accuracy = calculate_accuracy(tune, nl);
                     //tune
                     if (tune_accuracy > 0.62){
                    // System.out.println("tune_accuracy: " + tune_accuracy);
                         break;
                     }
                     if((tune_accuracy < prev_tune_accuracy) && (tune_accuracy > 0.60) ){
                         if(count > 2){
                   //  System.out.println("tune_accuracy: " + tune_accuracy);
                             break;
                         }

                         else{
                             count++;
                         }
                     }

                     prev_tune_accuracy = tune_accuracy;
                    // System.out.println("tune_accuracy: " + tune_accuracy);

                     // reset the learning rate
                     alpha = origin_alpha;
             }

             Double test_accuracy = calculate_accuracy(test, nl);
         //    System.out.println("test accuracy: " + test_accuracy + " with count: " + count);
             FileWriter outfile = new FileWriter(new File("lab2_data.csv"), true);
             outfile.write(test_accuracy.toString()+"\n" );
             outfile.write(Integer.toString(i) + "\n");
             outfile.close();
        }
    }




    protected static double calculate_accuracy(List<Strian> test, List<ANN> nl) {
        int correct = 0 ;
        int start = 0, end = windowSize - 1;
        int n = 0;
       // System.out.println("test set size: " + test.size());
        for (Strian s : test) {
            n += s.ps.size();
        }
        for(Strian s : test){
            List<dfvec>  raw_inputs = convertToOneHotFvec(s);
            start = 0;
            end = windowSize - 1;
            while(end < raw_inputs.size()){
                char label = raw_inputs.get((start + end + 2)/2).label;
                List<Character>  voteoutput= new ArrayList<Character>();
                for (ANN ann : nl) {
                    List<dfvec> in = raw_inputs.subList(start, end + 1);
                    ann.Calculate_Output(in);
                    int maxidx = 0;
                    double max = 0;
                    for (netPerceptron p : ann.output_layer) {
                        if(p.output > max){
                            maxidx = ann.output_layer.indexOf(p);
                            max = p.output;
                        }
                    }
                    char output = ' ';
                    if(maxidx == 0){
                        output = '_';
                    }
                    else if(maxidx == 1){
                        output = 'e';
                    }
                    else if (maxidx == 2){
                        output = 'h';
                    }
                    else{
                        System.exit(11);
                    }
                    voteoutput.add(output);
                    ann.clear_input_output_delta();
            }
                // ensemble final output
                char enout = majority(voteoutput);
             if (enout == label){
                    correct++;
                }
                else{
                }

                start++;
                end++;
        //        //  System.out.println("output: " + output + " label: " + label);
            }

        }


        return correct*1.0/n;
    }

    private static char majority(List<Character> l) {
        Map<Character, Integer> map = new HashMap<>();
        for (Character c : l) {
            Integer val = map.get(c);
            map.put(c, val == null ? 1 : val + 1);
        }

        Map.Entry<Character, Integer> max = null;
        for (Map.Entry<Character, Integer> e : map.entrySet()) {
            if (max == null || e.getValue() > max.getValue())
                max = e;
        }

        return max.getKey().charValue();
    }


    protected static  List<dfvec>  convertToOneHotFvec(Strian sample) {
        List<dfvec> dfvecs = new ArrayList<dfvec>();
        //List<dfvec> updateset  = new ArrayList<dfvec>();

        //pad first padNum: 8

        for(int i  = 0 ; i < padNum; i++){
        dfvec solvent = new dfvec('0', 21, "S");
        dfvecs.add(solvent);
        }
             char label = ' ';
        for (pair p : sample.ps) {
            char acid_c = p.acid.charAt(0);

            dfvec curFeatureVector = new dfvec(acid_c, acidNum, p.label);
            dfvecs.add(curFeatureVector);
        }

        // pad last padNum: 8
        for(int i  = 0 ; i < padNum; i++){
            dfvec solvent = new dfvec('0', 21, "S");
            dfvecs.add(solvent);
        }

        return dfvecs;
    }

    private static List<Strian> ParseFile(String fileName) throws IOException {
        if(fileName == null || fileName.equals("")){
            System.out.println("file name is null or empty");
            System.exit(10);
        }

        String [] lines =  null;
        // Files.lines(Paths.get(fileName), StandardCharsets.UTF_8);
        List<Strian> samples = new ArrayList<Strian>();
        try{
            Stream <String> s = Files.lines(Paths.get(fileName));
            // convert into String arrays
            lines = s.toArray(size -> new String[size]);
            List<String> l = Arrays.asList(lines);
            samples = extractStrains(l);

            // System.out.println(samples);

        } catch (IOException e) {
            e.printStackTrace();
        }
        return samples;
    }

    private static List<Strian> extractStrains(List<String> raw_input) {
        // split according to "<>"
        List<Strian> samples = new ArrayList<>();
        // pair p = null;
        List <String>acids = new ArrayList<String>();
        List <String>labels = new ArrayList<String>();
        for (String s : raw_input) {
            if(s.trim().startsWith("#") || s.equals("")){
                continue;
            }
            if(s.trim().equals("<>")){
                // crate new pair by clearing the previous elements in acids
                //System.out.println("<>" );
                if(!acids.isEmpty() && !labels.isEmpty()){
                    Strian cursample = new Strian(acids, labels);
                    samples.add(cursample);
                    acids.clear();
                    labels.clear();
                }

            }
            else if(s.trim().equals("end") || s.trim().equals("<end>")){
                // add created one
                continue;
                /*Strian cursample = new Strian(acids, labels);
                samples.add(cursample);
                acids.clear();
                labels.clear();*/
                //System.out.println("<end>" );
            }
            else{
                String[] fields = s.trim().split(" ");
                if(fields.length!=2){
                    System.out.println("splied line size is not 2 (acid + label)");
                    System.exit(2);
                }
                if(fields[0].equals("")) {
                    System.out.println("fields[0] = \"\",missing acid letter in this line ");
                    System.exit(3);
                }
                if(fields[1].equals("")) {
                    System.out.println("fields[1] = \"\",missing label in this line ");
                    System.exit(4);
                }

                acids.add(fields[0]);
                labels.add(fields[1]);
                //System.out.println(fields[0] + " " + fields[1]);
            }

        }
        return samples;
    }
}


class ANN extends Lab2{
    static double dropout_rates = 1/3;
    List<netPerceptron> input_layer;
    List<netPerceptron> hidden_layer;
    List<netPerceptron> output_layer;
    public ANN() {
        // build network
        // forward
        input_layer  = new ArrayList<netPerceptron>();
        for (int i = 0 ; i < INPUT_LAYER_NUM ; i ++){
            netPerceptron p = new netPerceptron(1,null,input_type_Nums);
            input_layer.add(p);
        }

        hidden_layer  = new ArrayList<netPerceptron>();
        for(int i = 0 ; i < HIDDEN_LAYER_NUM; i ++){
            netPerceptron p = new netPerceptron(2,input_layer,null);
            hidden_layer.add(p);
        }

        for (netPerceptron perceptron : input_layer) {
            perceptron.nextLayer = hidden_layer;
        }
        for (netPerceptron perceptron : hidden_layer) {
            perceptron.prevLayer = input_layer;
        }

        output_layer  = new ArrayList<netPerceptron>();
        for (int i = 0; i < OUTPUT_LAYER_NUM; i++) {
            netPerceptron p = new netPerceptron(3,hidden_layer);
            output_layer.add(p);
        }

        for (netPerceptron perceptron : output_layer) {
            perceptron.prevLayer = hidden_layer;
        }
        for (netPerceptron perceptron: hidden_layer){
            perceptron.nextLayer = output_layer;
        }
    }

    public void dropout(int layer){
        if(layer != 1 && layer !=2){
            System.out.println("layerNum: " + layer);
            System.exit(12);
        }

        // Randomly Drop out Perceptron on hidden layer
        Random r = new Random();
        //int hiddenDropOutNum = r.nextInt(HIDDEN_LAYER_NUM/2);
        if(layer == 1){

        for(int i= 0 ; i < HIDDEN_LAYER_NUM* dropout_rates; i++){
            int target = r.nextInt(HIDDEN_LAYER_NUM);
            // reset the weight for the target perceptron
            netPerceptron p = hidden_layer.get(target);
            // reset backward
            for (int j = 0; j < p.weights.length; j++) {
                p.weights[i] = 0;
            }
            // reset forward
            for (netPerceptron parent : p.nextLayer) {
                parent.weights[hidden_layer.indexOf(p)] = 0;
            }
        }
        }

        else{
        // Randomly Dropout Perceptron on input layer
        for(int i= 0 ; i < INPUT_LAYER_NUM* dropout_rates; i++){
            int target = r.nextInt(INPUT_LAYER_NUM);
            // reset the weight for the target perceptron
            netPerceptron p = input_layer.get(target);
            // reset backward
            for (int j = 0; j < p.weights.length; j++) {
                p.weights[i] = 0;
            }
            // reset forward
            for (netPerceptron parent : p.nextLayer) {
                parent.weights[hidden_layer.indexOf(p)] = 0;
            }
        }
        }
        // scale the rest of weights by
    }

    public void training(List<Strian> train) {
        for (Strian strian : train) {
            List<dfvec>  raw_inputs = convertToOneHotFvec(strian);
            int max_batchNum = raw_inputs.size() - (windowSize - 1);
            int real_batchNum = Math.min(batchNum,max_batchNum);

            if(real_batchNum < batchNum ){
                System.out.println("batchNum greater than maximum possible number of updates in " +
                        "in the current strain, so make the batchNum as " +
                        " the size of total number of movings for the input window, which the maximum number of " +
                        "batchNum for each strain (protein sequence) can take ");
                batchNum = real_batchNum;
            }
            //     if(raw_inputs.isEmpty()) System.out.println("raw_inputs empty");
            List<List<dfvec>> batch = new ArrayList<>();
            int start = 0, end = windowSize - 1;

            while(end < raw_inputs.size()){
                if(start % real_batchNum == 0 && start != 0){
                    update(batch);
                    batch.clear();
                }

                List<dfvec> sample = raw_inputs.subList(start, end + 1);
                batch.add(sample);
                start++;
                end++;

               /* count++;
                netPerceptron dashabi = input_layer.get(0);
                //System.out.println( "input: " + dashabi.netinput + " | " + "output: "+ dashabi.output +  " | " +  "delta: " + dashabi.delta);
                System.out.println( Arrays.toString(dashabi.weights));*/

            }

        }
    }
    public void update(List<List<dfvec>> samples) {

        if(samples == null || samples.size() == 0 ){
            System.out.println("empty samples list");
            System.exit(10);
        }
        List<Character> labels  = Arrays.asList('_','e','h');

        List<dfvec> input_avg = new ArrayList<>(samples.get(0));

        // forward
        double[] errs  = new double[OUTPUT_LAYER_NUM];
        for (List<dfvec> sample : samples) {
            if(samples.indexOf(sample) != 0){
                for (dfvec f : input_avg) {
                    f.add(sample.get(input_avg.indexOf(f)));
                }
            }
            Calculate_Output(sample);
            // calculate and update the errs
            // get the label: teacher vector
            double[] teacher = new double[OUTPUT_LAYER_NUM];
            teacher[labels.indexOf(sample.get((windowSize + 1)/2).label)] = 1.0;

            for (int i = 0; i < errs.length; i++) {
                netPerceptron p = output_layer.get(i);
                errs[i] += teacher[i] - p.output;
            }
        }
        // average the sum to get the avg input (for batch update)
        /*for (dfvec f : input_avg) {
            for (int i = 0; i < f.features.length; i++) {
                f.features[i]/=(samples.size());
            }
        }*/
        //System.out.println(input_avg);
       /* for (int i = 0; i < errs.length; i++) {
            System.out.print(errs[i] + " ");
        }*/

        // backprop delta
        int count =0;
        for (netPerceptron op : output_layer) {
            op.delta = op.sigmoidP(op.output)*errs[output_layer.indexOf(op)];
            // update next layer delta
            for (int i = 0; i < op.weights.length - 1; i++) {
                netPerceptron child = op.prevLayer.get(i);
                child.delta += child.ReLUP(child.netinput)*op.weights[i] * op.delta;
                // update output layer weights
                op.weights[i] += alpha * op.delta * child.output;
               
                if(count <5)
                {               // System.out.println("delta " + op.delta+  " op.lastdelta"+op.lastdelta);
                count++;
                }
                // System.out.println(op.lastdelta);
            }
            // bias
            op.weights[op.weights.length - 1] += alpha*op.delta*(-1);
        }

        for (netPerceptron hp : hidden_layer) {
            /*double weightedSum = 0;
            int index = hidden_layer.indexOf(hp);
            for (netPerceptron np : output_layer) {

            }*/
           /* hp.delta = hp.delta*hp.ReLUP(hp.netinput);*/

            for (int i = 0; i < hp.weights.length - 1; i ++) {
                netPerceptron chlid = hp.prevLayer.get(i);
                chlid.delta += hp.weights[i]*hp.delta;
                // update weights
                hp.weights[i] += (alpha * hp.delta* chlid.output) ;
                hp.weights[i] += hp.lastdelta * beta;
            }

            // bias
            hp.weights[hp.weights.length - 1] += alpha*hp.delta*(-1);
        }
        for (netPerceptron ip : input_layer) {
            int index = input_layer.indexOf(ip);
            dfvec input = input_avg.get(index);
            for (int i = 0; i < ip.weights.length; i++) {
                ip.weights[i] += alpha * ip.delta * input.features[i];
            }
        }

        clear_input_output_delta();

    }

    public void Calculate_Output(List<dfvec> sample) {
        double [] output = new double[OUTPUT_LAYER_NUM];
        //System.out.println("sample size: " + sample.size() + "input_layer: " + input_layer.size());
        // input layer
        for(dfvec featureVec : sample){
            int i = sample.indexOf(featureVec);
            netPerceptron p = input_layer.get(i);
            p.CalculateWeightedSum(featureVec.features);
            // input layer: output = input
            p.output = p.netinput;
        }

        // hidden layer
        for (netPerceptron hp : hidden_layer) {
            for (netPerceptron child : hp.prevLayer) {
                hp.netinput += child.output*hp.weights[hp.prevLayer.indexOf(child)];
            }
            hp.output = hp.ReLU(hp.netinput);
        }

        //output
        for (netPerceptron op : output_layer) {
            for (netPerceptron child : hidden_layer) {
                op.netinput += child.output*op.weights[hidden_layer.indexOf(child)];
            }
            op.output = op.sigmoid(op.netinput);
        }

    }
    public void clear_input_output_delta() {
        // clear input and output and delta for each perceptron
        for (netPerceptron p : input_layer) {
            p.output = 0;
            p.netinput = 0;
            p.lastdelta = p.delta;
            p.delta = 0;
            
        }
        for (netPerceptron p : hidden_layer) {
            p.output = 0;
            p.netinput = 0;
            p.lastdelta = p.delta;
            p.delta = 0;
        }
        for (netPerceptron p : output_layer) {
            p.output = 0;
            p.netinput = 0;
            p.lastdelta = p.delta;
            p.delta = 0;
        }
    }

}
class Strian {
    static int ID = 1;
    List<pair> ps;
    public Strian(List<String> acids, List<String> labels) {
        ps = new ArrayList<pair>();
        if(acids.size()!= labels.size()){
            System.out.println("acids list and labels are in different size, "
                    + "acids.size(): " + acids.size() + "labels.size(): " + labels.size()
            );
            System.exit(1);
        }

        for (int i = 0; i < acids.size(); i++) {
            pair p = new pair (acids.get(i), labels.get(i));
            ps.add(p);
        }
        ID ++;

    }

    public Strian(List<pair> ps) {
        this.ps = ps;
    }

    @Override
    public String toString() {
        String out = "ID: " + ID;
        for (pair p : ps) {
            out += p.toString();
        }
        return out;
    }
}

class pair{
    // static int count = 10;
    String acid;
    String label;

    public pair(String acid, String label) {
        this.acid = acid;
        this.label = label;
    }

    @Override
    public String toString() {
        return  acid + " " + label + " " + "\n";
    }
}

class dfvec{
    static HashMap<Character, Integer> dic = new HashMap<>();
    static int index = 0;
    int flen;
    double features[];
    char label;
    public dfvec(char input, int flen, String label) {
        //check if acid type exists
        int indexOfOne = 0 ;
        if(!dic.containsKey(input)){
            dic.put(input,index);
            index++;
        }
        indexOfOne = dic.get(input);
        features = new double[flen];
        features[indexOfOne] = 1.0;
        this.flen = flen;
        label = label.trim();
        if((label == null) || (label.length() != 1)){
            System.out.println("string label not a single char: " + label);
            System.exit(8);
        }
        this.label = label.charAt(0);
    }
    public dfvec add(dfvec other){
        for (int i = 0; i < features.length; i++) {
             features[i] += other.features[i];
        }
        return  this;
    }

    @Override
    public String toString() {
        return "dfvec{" +
                "flen=" + flen +
                ", features=" + Arrays.toString(features) +
                '}';
    }
}
class data {
    List<dfvec> input;
    char label;

    public data(List<dfvec> input, char label) {
        this.input = input;
        this.label = label;
    }
}

class netPerceptron extends Lab2{
    static double alpha = 0.01;
    double [] weights;
    List<netPerceptron> nextLayer;
    List<netPerceptron> prevLayer;
    int layerNum;
    double netinput;
    double output;
    double delta;
    double lastdelta;
    // layerNum 1: raw_input layer's perceptron constructor: without prevLayer
    public netPerceptron(int layerNum, List<netPerceptron> nextLayer, int input_Num) {
        this.layerNum = layerNum;
        if(layerNum == 1){
            this.nextLayer = nextLayer;
        }
        else{
            System.out.println("layerNum number error, 1: input 2: hidden(ReLu) 3: output(sigmoid)");
            System.out.println("expected layerNum: 1, + actual layerNum: " + layerNum);
            System.exit(7);
        }

        // initialize weights
        Random r = new Random();
        weights = new double[input_Num];
        for (int i = 0; i < weights.length; i++) {
            weights[i] = (2*r.nextDouble() - 1) * 4/(Math.sqrt(6*(input_type_Nums + HIDDEN_LAYER_NUM))) ;
        }

        // generate random weight value for bias
      /*  Random r = new Random();
        weights[input.features.length] = r.nextDouble();*/

    }

    // layerNum 4: output(sigmoid as activation function) layer's perception constructor: without nextLayer
    public netPerceptron(int layerNum, List<netPerceptron> prevLayer) {
        // generate using random Generators
        if(layerNum == 3){
            this.prevLayer = prevLayer;
        }
        else{
            System.out.println("layerNum number error, 1：input 2: hidden(ReLu) 3: output(sigmoid)");
            System.out.println("expected layerNum: 3, + actual layerNum" + layerNum);
            System.exit(7);
        }

        // initialize weights
        Random r = new Random();
        weights = new double[prevLayer.size() + 1];
        for (int i = 0; i < weights.length; i++) {
            weights[i] = (2 * r.nextDouble() - 1)*(4/(Math.sqrt(6*HIDDEN_LAYER_NUM)));
        }
    }

    public netPerceptron(int layerNum, List<netPerceptron> prevLayer, List<netPerceptron> nextLayer) {
        if((layerNum == 2) ){
            this.prevLayer = prevLayer;
            this.nextLayer = nextLayer;
        }
        else{
            System.out.println("layerNum number error, 1：input 2: hidden(ReLu) 3: output(sigmoid)");
            System.out.println("expected layerNum: 2, + actual layerNum" + layerNum);
            System.exit(7);
        }

        this.layerNum = layerNum;
        // initialize weights : forward
        Random r = new Random();
        weights = new double[prevLayer.size() + 1];
        for (int i = 0; i < weights.length; i++) {
            weights[i] = (2 * r.nextDouble() - 1) * (4/Math.sqrt(6*(INPUT_LAYER_NUM + OUTPUT_LAYER_NUM)));
        }

    }

    public void addnextLayers(List<netPerceptron> nextLayer){
        this.nextLayer = nextLayer;
    }

    public void prevLayer(List<netPerceptron> prevLayer){
        this.prevLayer = prevLayer;
    }

    public void CalculateWeightedSum(double[] input){
        if(input.length != weights.length){
            System.out.println("input and weights size does not match");
            System.out.println("input: " + input.length + "weight: " + weights.length);
            System.exit(8);
        }
        double sum = 0;
        for (int i = 0; i < input.length; i++) {
            sum += input[i] * weights[i];
        }

        // bias weight
       /* sum+= (-1)*weights[input.length ];*/
        netinput = sum;

        // return sum;
    }

    public void updateweights (double[] input, double[] delta){

        //TODO

    }
    public double sigmoid (double x){
        return 1/(1 + (Math.exp(-x)));
    }
    public double sigmoidP (double x){
        return x*(1-x);
    }
    public double ReLU (double x){

        return Math.max(0,x);
    }
    public double ReLUP(double x){
        // TODO
        return x > 0 ? 1: 0;
    }

}
